defmodule NAITest do
  use ExUnit.Case
  doctest NAI

  defmodule UserDefinedTask do
    def execute_plan() do
      :task_struct
    end
  end

  def system_msg_do_plan(_) do
    nil
  end

  test "greets the world" do
    assert NAI.hello() == :world
  end

  test "interface stub" do
    system_msg_1 = nil
    system_msg_2 = nil
    user_msg_1 = nil
    user_msg_2 = nil
    llm_msg_1 = nil
    a = NAI.chat()
        |> NAI.Chat.message(system_msg_1)
        |> NAI.Chat.message(user_msg_1)
        |> NAI.Chat.message(llm_msg_1)
        |> NAI.Chat.message(user_msg_2)
        |> NAI.Chat.tag(:base)
        |> NAI.Chat.message(system_msg_2)
        |> NAI.Chat.with_model(NAI.Model.best(for: :plan))
        |> NAI.Chat.set_stream(true, required: true)
        |> NAI.Chat.tag(:plan)
    assert %NAI{} = a

    a_outcome = NAI.ChatResponse.body(a)
    assert a_outcome == :nyi

    b = NAI.Chat.checkout(a, :base)
        |> NAI.Chat.message(system_msg_do_plan(a_outcome))
        |> NAI.Chat.with_model(NAI.Model.fastest(for: UserDefinedTask.execute_plan(), params: :required))
        |> NAI.Chat.tag(:execute)
    b_outcome = NAI.ChatResponse.body(b)
    assert b_outcome == :nyi
  end
  #
  # NAI.list_models(subject \\ %NAI{})
  # NAI.provider(subject \\ %NAI{})

  # list models:
  # 1.
  # NAI.provider()
  # |> NAI.list_models()
  # 2.
  # NAI.list_models()
  # 3.
  # NAI.list_models(NAI.providers([NAI.Gemini, NAI.OpenAI]))
  # 3.
  # NAI.list_models(NAI.Gemini)

  #
  # Overly Complex Example - chat completion
  #

  # NAI.transaction do # Loads
  #     a = NAI.chat()
  #         |> with_model(NAI.Gemini.Model.ultra) # optional will check configured defaults otherwise for action type.
  #         # |> with_model(NAI.OpenAI.Model.custom(name: "ultraultra", ...)) # of we are manually defining a model or fine tuned model.
  #         # |> with_model(CustomProvider.custom_model) # if we had a user defined provider
  #         # |> with_model(NAI.Model.cheapest(params: :best_effort)) # if we want to pick the cheapest model at inference time based on context window, complexity, params. (will ignore params if they restrict a cheaper model.)
  #         # |> with_model(NAI.Model.fastest()) # if we want to pick the cheapest model at inference time based on context window, complexity, params. (will ignore params if they restrict a cheaper model.)
  #         # |> with_model(NAI.Model.smartest(for: :problem_type)) # if we want to pick the cheapest model at inference time based on context window, complexity, params.
  #         |> message(system_prompt) # input is a message object or a lambda for deferred invocation.
  #         |> message(user_message)
  #         |> message(agent_response)
  #         |> message(user_message_2)
  #         |> tag(:base)
  #         |> message(system_prompt_prepare_plan)
  #         |> tag(:plan)
  #         |> checkout(:base)
  #         |> then(& message(prepare_system_message_do_plan(NAI.ChatResponse.body(checkout(&1, :plan)))) # syntactic sugar inline invocation and response extraction to add new message from message generated by previous thread at tag
  #         |> message(&NAI.ChatResponse.body/2, as: :assistant) # when executed will wait for response of thread and append additional message with output.
  #         |> tag(:response)
  #         |> message(system_prompt_reflect)
  #         |> tag(:fin)
  #         |> set_stream(true, required: true) # if dynamic model selection force this setting to be honored, ignore models with out support.
  #     b = NAI.ChatResponse.body(a) # if non stream fetch's objects completion field {:fin, x}
  #                                  # if a stream wrapper yields until some completion value available or request failure and returns {{:stream, tag, seq}, wip} or {:fin, x} or error
  #                                  # todo mechanism to fetch intermediates - e.g. stream and retrieve each tagged response after execution response, requires changes but here we'd want the output of the plan, response, and review/reflect
  #     NAI.ChatResponse.yield(a, 5_000) # Yield till done or 5_000
  #     {NAI.ChatResponse.done?(a), NAI.ChatResponse.tokens(a)} # all we return from this "transaction" is done flag and tokens so far.
  # end

  #
  # Slightly Simpler Example - chat completion, no txn
  #

  # require NAI.Chat
  # chat     = NAI.chat()
  #            |> NAI.Chat.message(system_prompt) # input is a message object or a lambda for deferred invocation.
  #            |> NAI.Chat.message(user_message)
  #            |> NAI.Chat.message(agent_response)
  #            |> NAI.Chat.message(user_message_2_with_image)
  #            |> NAI.Chat.tag(:base) # --> NAI.tag(:base)
  #            |> NAI.Chat.message(system_prompt_prepare_plan)
  #            |> NAI.Chat.with_model(NAI.Model.best(for: :plan))
  #            |> NAI.Chat.set_stream(true, required: true))
  #            |> NAI.Chat.tag(:plan)
  #            |> then(& IO.inspect(NAI.list_models(&1)) && &1) # output models sorted by how well they meet constraints - using local cache not remote query.
  # plan = NAI.ChatResponse.body(chat)
  # chat2 = NAI.Chat.checkout(chat, :base) # behind the scenes a command tree is built.
  #         |> NAI.Chat.message(prepare_system_message_do_plan(plan))
  #         |> NAI.Chat.with_model(NAI.Model.fastest(for: UserDefinedTask.execute_plan(), params: :required)) # pick fastest model a user defined task require any set settings be honored.
  #         |> NAI.Chat.tag(:execute)
  # Enum.map(NAI.ChatResponse.stream(chat2), &IO.inspect/1) # stream is a lazy enumerable, we can map over it to get each response.
  #     => :sending
  #     => {:stream, {:execute, 1}, "Ipso Lorem Ipsum"}
  #     => {:stream, {:execute, 5}, "Ipso Lorem Ipsum Dol"}
  #     => {:fin, "Ipso Lorem Ipsum Dolor Sit Amet"
  # #alt
  # NAI.ChatResponse.stream(chat2, &handler/3, options \\ nil) # handler(%NAI{}, feed :: tuple, options :: any \\ nil)

  # NAI struct
  # tree of instructions a -> b -> c -> tag(1) -> d -> e
  #                                   \-> f -> g -> (tag2) -> h -> i
end
